{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ddffd4-a198-4707-9056-fb58923c3332",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39dff0-cd80-4e6e-ac77-b8fa1be44f6a",
   "metadata": {},
   "source": [
    "### downloading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06152893-2961-4dec-a950-989809f517a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c967826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of classes in yoga-82 text files\n",
    "class_names_82 = [\n",
    "    \"Child_Pose_or_Balasana_\",\n",
    "    \"Warrior_II_Pose_or_Virabhadrasana_II_\",\n",
    "    \"Standing_Split_pose_or_Urdhva_Prasarita_Eka_Padasana_\",\n",
    "    \"Downward-Facing_Dog_pose_or_Adho_Mukha_Svanasana_\",\n",
    "    \"Dolphin_Pose_or_Ardha_Pincha_Mayurasana_\",\n",
    "    \"Cobra_Pose_or_Bhujangasana_\",\n",
    "    \"Tree_Pose_or_Vrksasana_\",\n",
    "    \"Cow_Face_Pose_or_Gomukhasana_\",\n",
    "    \"viparita_virabhadrasana_or_reverse_warrior_pose\"\n",
    "    \n",
    "]\n",
    "\n",
    "# Names of classes in my dataset\n",
    "class_names = [\n",
    "    \"Child\",\n",
    "    \"Warrior_II\",\n",
    "    \"Standing_Split\",\n",
    "    \"Downward_dog\",\n",
    "    \"Dolphin\",\n",
    "    \"Cobra\",\n",
    "    \"Tree\",\n",
    "    \"Cow_Face\",\n",
    "    \"Reverse_Warrior\"\n",
    "]\n",
    "\n",
    "Yoga_82_path = \"../yoga-82/yoga_dataset_links/\"\n",
    "data_dir = \"../dataset/\"\n",
    "intermediary_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87390c28-2eb6-4329-abba-c8d84916fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 500 files\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Den angivne sti blev ikke fundet: 'Child/Child_0.jpg' -> '../dataset/Child/Child_0.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\osteb\\OneDrive - Aarhus universitet\\Kandidat\\10 semester\\Deep neural networks for visual recognision\\YogaPoseClassifier\\Preprocessing\\rcnn.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished reading files for: \u001b[39m\u001b[39m\"\u001b[39m, class_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(intermediary_file_name)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m DowloadData()\n",
      "\u001b[1;32mc:\\Users\\osteb\\OneDrive - Aarhus universitet\\Kandidat\\10 semester\\Deep neural networks for visual recognision\\YogaPoseClassifier\\Preprocessing\\rcnn.ipynb Cell 5\u001b[0m in \u001b[0;36mDowloadData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Move data from intermediary file to dataset dir\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m filenames:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         os\u001b[39m.\u001b[39;49mreplace(intermediary_file_name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m filename, data_dir \u001b[39m+\u001b[39;49m class_name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m\"\"\"         for filename in test:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m        os.replace(intermediary_file_name + \"/\" + filename, data_dir + \"Test/\" + class_name + \"/\" + filename)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    for filename in val:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m        os.replace(intermediary_file_name + \"/\" + filename, data_dir + \"Validation/\" + class_name + \"/\" + filename) \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished reading files for: \u001b[39m\u001b[39m\"\u001b[39m, class_name)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Den angivne sti blev ikke fundet: 'Child/Child_0.jpg' -> '../dataset/Child/Child_0.jpg'"
     ]
    }
   ],
   "source": [
    "def DowloadData():\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for class_name in class_names:\n",
    "        try:\n",
    "            os.mkdir(class_name)\n",
    "        except:\n",
    "            pass    \n",
    "\n",
    "\n",
    "    for i in range(len(class_names_82)):\n",
    "        counter = 0\n",
    "        class_name = class_names[i]\n",
    "        name_82 = class_names_82[i]\n",
    "        intermediary_file_name = intermediary_path + class_name\n",
    "        try:\n",
    "            os.mkdir(intermediary_file_name)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        url_file = Yoga_82_path + \"/\" + name_82 + \".txt\"\n",
    "        filenames = os.listdir(os.getcwd() + \"/\" + class_name)\n",
    "        with open(url_file) as f:\n",
    "            d = f.readline()\n",
    "            while d:\n",
    "                url = d.split('\\t')[1].strip('\\n')\n",
    "                filename = class_name + '_' + str(counter) + \".jpg\"\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, intermediary_file_name + \"/\" + filename)\n",
    "                    filenames.append(filename)\n",
    "                except:\n",
    "                    pass\n",
    "                counter += 1\n",
    "                d = f.readline()\n",
    "\n",
    "        print(\"Read \" + str(counter) + \" files\\n\")\n",
    "\n",
    "    # Randomly split dataset into train, test and validation using 60-20-20 split\n",
    "        train_val, test = train_test_split(filenames, test_size=0.2, train_size=0.8)\n",
    "        train, val = train_test_split(train_val, test_size=0.25, train_size=0.75)\n",
    "\n",
    "    # Move data from intermediary file to dataset dir\n",
    "        for filename in filenames:\n",
    "            os.replace(intermediary_file_name + \"/\" + filename, data_dir + class_name + \"/\" + filename)\n",
    "\n",
    "        \"\"\"         for filename in test:\n",
    "            os.replace(intermediary_file_name + \"/\" + filename, data_dir + \"Test/\" + class_name + \"/\" + filename)\n",
    "\n",
    "        for filename in val:\n",
    "            os.replace(intermediary_file_name + \"/\" + filename, data_dir + \"Validation/\" + class_name + \"/\" + filename) \"\"\"\n",
    "    \n",
    "        print(\"Finished reading files for: \", class_name)\n",
    "\n",
    "    \n",
    "    for x in class_names:\n",
    "        shutil.rmtree(x)\n",
    "\n",
    "DowloadData()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c863377-b2d9-45fd-9667-0ca361192f3d",
   "metadata": {},
   "source": [
    "### Preparing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42850282-0db6-4c85-98e9-b0e6abd031e3",
   "metadata": {},
   "source": [
    "The input to the model is expected to be a list of tensors, each of \n",
    "- shape [C, H, W], one for each image \n",
    "- Intensity range 0-1. \n",
    "- Different images can have different sizes.   \n",
    "\n",
    "The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss.\n",
    "\n",
    "During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows, where N is the number of detected instances:\n",
    "\n",
    "boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H.\n",
    "\n",
    "labels (Int64Tensor[N]): the predicted labels for each instance\n",
    "\n",
    "scores (Tensor[N]): the scores or each instance\n",
    "\n",
    "keypoints (FloatTensor[N, K, 3]): the locations of the predicted keypoints, in [x, y, v] format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7781a",
   "metadata": {},
   "source": [
    "Images can be correctly formatted by using the transform that is included with the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb8d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = KeypointRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "img_to_ipt = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565cd8a",
   "metadata": {},
   "source": [
    "### Data augmentation (setting up data loaders)\n",
    "We apply data augmentation to the training set and use the default transformer for the test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39beff24-114f-49c3-9f77-0e30f96397df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and img preprocessing\n",
    "data_transforms = {\n",
    "    'Train': transforms.Compose([\n",
    "        img_to_ipt,\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(0.5), \n",
    "        transforms.ColorJitter(brightness=0.1),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Validation': img_to_ipt,\n",
    "    'Test': img_to_ipt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00888ee2",
   "metadata": {},
   "source": [
    "Using the defined transformers, we can load the training, validation and test dataset into dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d652f6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\\n                                             shuffle=True, num_workers=4)\\n              for x in dset_labels}\\ndataset_sizes = {x: len(image_datasets[x]) for x in dset_labels}\\nclass_names = image_datasets['Train'].classes \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_labels = ['Train', 'Validation', 'Test']\n",
    "\n",
    "\"\"\" image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in dset_labels} \"\"\"\n",
    "image_datasets = datasets.ImageFolder(data_dir+'Train',\n",
    "                            data_transforms['Train'])\n",
    "\"\"\" dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in dset_labels}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in dset_labels}\n",
    "class_names = image_datasets['Train'].classes \"\"\"\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bf8127",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\osteb\\OneDrive - Aarhus universitet\\Kandidat\\10 semester\\Deep neural networks for visual recognision\\YogaPoseClassifier\\Preprocessing\\rcnn.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m image_datasets[\u001b[39m1\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 230\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    246\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 248\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[0;32m    249\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\PIL\\Image.py:3101\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3098\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m   3099\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 3101\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[0;32m   3103\u001b[0m preinit()\n\u001b[0;32m   3105\u001b[0m accept_warnings \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "image_datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485bca4-1603-46d9-a255-f4614bcc0f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\osteb\\OneDrive - Aarhus universitet\\Kandidat\\10 semester\\Deep neural networks for visual recognision\\YogaPoseClassifier\\Preprocessing\\rcnn.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m title \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         plt\u001b[39m.\u001b[39mtitle(title)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m inputs, classes \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloaders[\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m out \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmake_grid(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m imshow(out, title\u001b[39m=\u001b[39m[class_names[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m classes])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(inp: torch.Tensor, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "inputs, classes = next(iter(dataloaders['Train']))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b149f18-3b63-497f-bda4-f60e02402f90",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729998f5-b4ea-4f6d-92b8-380cd4ac24ec",
   "metadata": {},
   "source": [
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.keypointrcnn_resnet50_fpn.html#torchvision.models.detection.keypointrcnn_resnet50_fpn\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f521462-79dc-4b1f-8a2b-b98170e6fe43",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[Errno 2] No such file or directory: 'child.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\osteb\\OneDrive - Aarhus universitet\\Kandidat\\10 semester\\Deep neural networks for visual recognision\\YogaPoseClassifier\\Preprocessing\\rcnn.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         axs[\u001b[39m0\u001b[39m, i]\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39masarray(img))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         axs[\u001b[39m0\u001b[39m, i]\u001b[39m.\u001b[39mset(xticklabels\u001b[39m=\u001b[39m[], yticklabels\u001b[39m=\u001b[39m[], xticks\u001b[39m=\u001b[39m[], yticks\u001b[39m=\u001b[39m[])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m img \u001b[39m=\u001b[39m read_image(\u001b[39m\"\u001b[39;49m\u001b[39mchild.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Step 1: Initialize model with the best available weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/osteb/OneDrive%20-%20Aarhus%20universitet/Kandidat/10%20semester/Deep%20neural%20networks%20for%20visual%20recognision/YogaPoseClassifier/Preprocessing/rcnn.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m weights \u001b[39m=\u001b[39m KeypointRCNN_ResNet50_FPN_Weights\u001b[39m.\u001b[39mDEFAULT\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torchvision\\io\\image.py:251\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    250\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 251\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torchvision\\io\\image.py:47\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m     46\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 47\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mread_file(path)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\osteb\\anaconda3\\envs\\MachineLearning\\lib\\site-packages\\torch\\_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: 'child.jpg'"
     ]
    }
   ],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "img = read_image(\"child.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = KeypointRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "img_float = transforms(img)\n",
    "model = keypointrcnn_resnet50_fpn(weights=weights, progress=False)\n",
    "model.eval()\n",
    "\n",
    "predictions = model(train_loader)\n",
    "\n",
    "# # Step 2: Initialize the inference transforms\n",
    "# preprocess = weights.transforms()\n",
    "\n",
    "# # Step 3: Apply inference preprocessing transforms\n",
    "# batch = [preprocess(img)]\n",
    "\n",
    "# # Step 4: Use the model and visualize the prediction\n",
    "# prediction = model(batch)[0]\n",
    "# labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "# box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "#                           labels=labels,\n",
    "#                           colors=\"red\",\n",
    "#                           width=4, font_size=30)\n",
    "# im = to_pil_image(box.detach())\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2738bbb-e6fd-4c6a-aa96-ef91e2fee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.meta[\"keypoint_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4411a-83d7-45ba-8732-3ca606790117",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts = predictions[0]['keypoints']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "print(kpts)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece1037-d35d-468f-9394-7b85ec9b4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_threshold = 0.85\n",
    "idx = torch.where(scores > detect_threshold)\n",
    "keypoints = kpts[idx]\n",
    "\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57984b-18f3-4427-8491-b306410fd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = draw_keypoints(img, keypoints, colors=\"blue\", radius=3)\n",
    "show(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc32b0-750a-4200-9cf3-47224bcfedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('MachineLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46ee0b6c7301175c84ffba450dc4ddbac263b471867139b3a823796b3a1ac9fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
